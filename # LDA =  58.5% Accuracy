# LDA =  58.5% Accuracy

# Boosted Logit Model= 72% when niter=31

# kNN = 65.61% Accuracy with k=5

# SVM  with RBF kernel= 62.83% Accuracy
# SVM with poly Kernel = 67.2%

# Random Forest = 72.76% Accuracy

# Naive Bayes when use Kernel is TRUE = 59%
# XG_boost = 73.44% Accuracy, overall accuracy= 72.13% 
# Deep neural network = 58.13% a 8-5-3 network with 63 weights
options were - softmax modelling  decay=0.1

#Probabilistic Neural Network = 48.22% Accuracy






################################   XGBoost ###########################################


eXtreme Gradient Boosting 

253 samples
  8 predictor
  3 classes: 'High', 'Low', 'Medium' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 228, 228, 227, 227, 227, 228, ... 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  Accuracy 
  0.3  1          0.6               0.50        50      0.6443077
  0.3  1          0.6               0.50       100      0.6800000
  0.3  1          0.6               0.50       150      0.6996923
  0.3  1          0.6               0.75        50      0.6875385
  0.3  1          0.6               0.75       100      0.6720000
  0.3  1          0.6               0.75       150      0.6920000
  0.3  1          0.6               1.00        50      0.6955385
  0.3  1          0.6               1.00       100      0.6760000
  0.3  1          0.6               1.00       150      0.6840000
  0.3  1          0.8               0.50        50      0.6643077
  0.3  1          0.8               0.50       100      0.6720000
  0.3  1          0.8               0.50       150      0.6763077
  0.3  1          0.8               0.75        50      0.6915385
  0.3  1          0.8               0.75       100      0.6800000
  0.3  1          0.8               0.75       150      0.6881538
  0.3  1          0.8               1.00        50      0.6915385
  0.3  1          0.8               1.00       100      0.6798462
  0.3  1          0.8               1.00       150      0.6761538
  0.3  2          0.6               0.50        50      0.6918462
  0.3  2          0.6               0.50       100      0.6764615
  0.3  2          0.6               0.50       150      0.7004615
  0.3  2          0.6               0.75        50      0.6724615
  0.3  2          0.6               0.75       100      0.6800000
  0.3  2          0.6               0.75       150      0.6603077
  0.3  2          0.6               1.00        50      0.6761538
  0.3  2          0.6               1.00       100      0.6803077
  0.3  2          0.6               1.00       150      0.6803077
  0.3  2          0.8               0.50        50      0.6603077
  0.3  2          0.8               0.50       100      0.6680000
  0.3  2          0.8               0.50       150      0.6681538
  0.3  2          0.8               0.75        50      0.6956923
  0.3  2          0.8               0.75       100      0.6921538
  0.3  2          0.8               0.75       150      0.6803077
  0.3  2          0.8               1.00        50      0.6958462
  0.3  2          0.8               1.00       100      0.6763077
  0.3  2          0.8               1.00       150      0.6843077
  0.3  3          0.6               0.50        50      0.6763077
  0.3  3          0.6               0.50       100      0.6963077
  0.3  3          0.6               0.50       150      0.7000000
  0.3  3          0.6               0.75        50      0.6804615
  0.3  3          0.6               0.75       100      0.6883077
  0.3  3          0.6               0.75       150      0.6724615
  0.3  3          0.6               1.00        50      0.6955385
  0.3  3          0.6               1.00       100      0.6955385
  0.3  3          0.6               1.00       150      0.6916923
  0.3  3          0.8               0.50        50      0.6880000
  0.3  3          0.8               0.50       100      0.6923077
  0.3  3          0.8               0.50       150      0.6884615
  0.3  3          0.8               0.75        50      0.6920000
  0.3  3          0.8               0.75       100      0.6843077
  0.3  3          0.8               0.75       150      0.6881538
  0.3  3          0.8               1.00        50      0.6840000
  0.3  3          0.8               1.00       100      0.6923077
  0.3  3          0.8               1.00       150      0.6840000
  0.4  1          0.6               0.50        50      0.6641538
  0.4  1          0.6               0.50       100      0.6804615
  0.4  1          0.6               0.50       150      0.6763077
  0.4  1          0.6               0.75        50      0.6796923
  0.4  1          0.6               0.75       100      0.6760000
  0.4  1          0.6               0.75       150      0.7001538
  0.4  1          0.6               1.00        50      0.6918462
  0.4  1          0.6               1.00       100      0.6720000
  0.4  1          0.6               1.00       150      0.6918462
  0.4  1          0.8               0.50        50      0.6989231
  0.4  1          0.8               0.50       100      0.6640000
  0.4  1          0.8               0.50       150      0.6643077
  0.4  1          0.8               0.75        50      0.6758462
  0.4  1          0.8               0.75       100      0.6683077
  0.4  1          0.8               0.75       150      0.6763077
  0.4  1          0.8               1.00        50      0.6918462
  0.4  1          0.8               1.00       100      0.6763077
  0.4  1          0.8               1.00       150      0.6684615
  0.4  2          0.6               0.50        50      0.6923077
  0.4  2          0.6               0.50       100      0.6886154
  0.4  2          0.6               0.50       150      0.6883077
  0.4  2          0.6               0.75        50      0.6724615
  0.4  2          0.6               0.75       100      0.6603077
  0.4  2          0.6               0.75       150      0.6686154
  0.4  2          0.6               1.00        50      0.6721538
  0.4  2          0.6               1.00       100      0.6647692
  0.4  2          0.6               1.00       150      0.6527692
  0.4  2          0.8               0.50        50      0.6878462
  0.4  2          0.8               0.50       100      0.6840000
  0.4  2          0.8               0.50       150      0.6806154
  0.4  2          0.8               0.75        50      0.6769231
  0.4  2          0.8               0.75       100      0.6926154
  0.4  2          0.8               0.75       150      0.6886154
  0.4  2          0.8               1.00        50      0.6800000
  0.4  2          0.8               1.00       100      0.6843077
  0.4  2          0.8               1.00       150      0.6724615
  0.4  3          0.6               0.50        50      0.6726154
  0.4  3          0.6               0.50       100      0.7001538
  0.4  3          0.6               0.50       150      0.6881538
  0.4  3          0.6               0.75        50      0.6846154
  0.4  3          0.6               0.75       100      0.6727692
  0.4  3          0.6               0.75       150      0.6723077
  0.4  3          0.6               1.00        50      0.6764615
  0.4  3          0.6               1.00       100      0.6841538
  0.4  3          0.6               1.00       150      0.6881538
  0.4  3          0.8               0.50        50      0.6958462
  0.4  3          0.8               0.50       100      0.6918462
  0.4  3          0.8               0.50       150      0.6958462
  0.4  3          0.8               0.75        50      0.7040000
  0.4  3          0.8               0.75       100      0.6995385
  0.4  3          0.8               0.75       150      0.6998462
  0.4  3          0.8               1.00        50      0.6878462
  0.4  3          0.8               1.00       100      0.6841538
  0.4  3          0.8               1.00       150      0.6841538
  Kappa    
  0.4498043
  0.5030783
  0.5327787
  0.5131685
  0.4913628
  0.5207987
  0.5279582
  0.4985660
  0.5092945
  0.4774783
  0.4884716
  0.4966240
  0.5190165
  0.5035394
  0.5169602
  0.5220313
  0.5056315
  0.4980514
  0.5184765
  0.4957367
  0.5340049
  0.4911047
  0.5020362
  0.4708580
  0.4972405
  0.5036912
  0.5039440
  0.4711889
  0.4804959
  0.4819548
  0.5277403
  0.5235738
  0.5053732
  0.5290205
  0.4999737
  0.5118678
  0.4951973
  0.5256348
  0.5318214
  0.5006525
  0.5144500
  0.4907813
  0.5270411
  0.5295599
  0.5252238
  0.5147355
  0.5215975
  0.5156390
  0.5220207
  0.5109439
  0.5163064
  0.5093501
  0.5242932
  0.5118354
  0.4753036
  0.5067241
  0.4978971
  0.5019350
  0.4967411
  0.5346649
  0.5228988
  0.4914290
  0.5222419
  0.5326044
  0.4764861
  0.4783221
  0.4966350
  0.4846973
  0.4994192
  0.5238380
  0.4995969
  0.4866632
  0.5231894
  0.5145103
  0.5123088
  0.4911975
  0.4722347
  0.4856017
  0.4883043
  0.4790165
  0.4621076
  0.5127686
  0.5076520
  0.5047474
  0.5001104
  0.5245901
  0.5178039
  0.5030628
  0.5118840
  0.4935537
  0.4898634
  0.5336505
  0.5150860
  0.5084179
  0.4916887
  0.4918492
  0.5013561
  0.5115691
  0.5170831
  0.5256466
  0.5190785
  0.5264960
  0.5393093
  0.5314744
  0.5324960
  0.5154335
  0.5122005
  0.5104423

Tuning parameter 'gamma' was held constant at a value of 0

Tuning parameter 'min_child_weight' was held constant at a value
 of 1
Accuracy was used to select the optimal model using the
 largest value.
The final values used for the model were nrounds = 50, max_depth
 = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8,
 min_child_weight = 1 and subsample = 0.75.




 ############################################################################




 Confusion Matrix and Statistics

          Reference
Prediction High Low Medium
    High     11   1      1
    Low       1  20      5
    Medium    2   6     14

Overall Statistics
                                         
               Accuracy : 0.7377         
                 95% CI : (0.6093, 0.842)
    No Information Rate : 0.4426         
    P-Value [Acc > NIR] : 2.836e-06      
                                         
                  Kappa : 0.5928         
                                         
 Mcnemar's Test P-Value : 0.9352         

Statistics by Class:

                     Class: High Class: Low Class: Medium
Sensitivity               0.7857     0.7407        0.7000
Specificity               0.9574     0.8235        0.8049
Pos Pred Value            0.8462     0.7692        0.6364
Neg Pred Value            0.9375     0.8000        0.8462
Prevalence                0.2295     0.4426        0.3279
Detection Rate            0.1803     0.3279        0.2295
Detection Prevalence      0.2131     0.4262        0.3607
Balanced Accuracy         0.8716     0.7821        0.7524